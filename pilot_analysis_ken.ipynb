{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838491ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hl_functions import *\n",
    "hl.init(log='./log.log')\n",
    "#hl.init(master='spark://c23n02.ruddle.hpc.yale.internal:7077')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628138fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from pcgc_hail.hail_scripts.utils import *\n",
    "#from new_names import *\n",
    "import gnomad\n",
    "from gnomad_methods import *\n",
    "from gnomad_methods.gnomad.sample_qc import *\n",
    "\n",
    "def hl_to_txt(hl_df, name, delim='\\t'):\n",
    "    \"\"\"Convert table to pandas dataframe and output to file\"\"\"\n",
    "    df = hl_df.to_pandas()\n",
    "    df.to_csv(name, sep=delim)\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "    \n",
    "import bokeh.io\n",
    "from bokeh.io import * \n",
    "from bokeh.layouts import *\n",
    "from bokeh.models import *\n",
    "hl.plot.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e553453",
   "metadata": {},
   "source": [
    "# User Updates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with desired paths \n",
    "\n",
    "# Make sure these directories have a slash at the end\n",
    "hl_outdir = hl_outdir = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/\"\n",
    "#dnv_outdir = \n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "#Yale generated Joint VCF based on Yale gvcfs (build 37)\n",
    "#vcf = '/gpfs/gibbs/pi/brueckner/Phil_Pipeline/yale_callset/yale_pipeline.vcf.gz'\n",
    "\n",
    "#Yale generated Joint VCF based on ACC gvcfs (build 38)\n",
    "#vcf = '/gpfs/gibbs/pi/brueckner/Phil_Pipeline/callset/phil_pipeline.vcf.gz'\n",
    "\n",
    "#ACC generated Joint VCF based on ACC gvcfs (build 38)\n",
    "#vcf = '/gpfs/gibbs/pi/brueckner/Phil_Joint_Calling_Pipeline/pilotcalls.vqsr.vcf.gz'\n",
    "\n",
    "#unsplit_mt = hl_outdir + 'unsplit.mt' \n",
    "split_mt = hl_outdir + 'split.mt' \n",
    "vep_mt = hl_outdir + 'vep.mt' \n",
    "vep_r1_mt = hl_outdir + 'vep_r1.mt' \n",
    "vep_r1c1_mt = hl_outdir + 'vep_r1c1.mt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d75c13",
   "metadata": {},
   "source": [
    "# VCF -> Matrix Table (unsplit) -> Matrix Table (split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17454dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VCF to hail matrix table \n",
    "\n",
    "#vcf = ###\n",
    "\n",
    "################################################################################\n",
    "print('Reads in:' + vcf)\n",
    "print('Writes out:' + mt_output)\n",
    "\n",
    "# if using genome 37\n",
    "#mt = hl.import_vcf(vcf, force_bgz=True, reference_genome='GRCh37')\n",
    "\n",
    "# if using genome 38\n",
    "# for whether to use recode see https://hail.is/docs/0.2/methods/impex.html#hail.methods.import_vcf\n",
    "recode = {f\"{i}\":f\"chr{i}\" for i in (list(range(1, 23)) + ['X', 'Y'])}\n",
    "mt = hl.import_vcf(vcf, force_bgz=True, reference_genome='GRCh38', contig_recoding=recode)\n",
    "\n",
    "# count before splitting \n",
    "print(str(mt.count()) + '\\n')\n",
    "\n",
    "# split multi-allelic sites  \n",
    "mt = generate_split_alleles(mt)\n",
    "\n",
    "# count after splitting \n",
    "print(str(mt.count()) + '\\n')\n",
    "\n",
    "mt.write(mt_output, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c819993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restrict to the exome intervals\n",
    "mt_input = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium/split2.mt\"\n",
    "mt_output = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/split_intervals_acc.mt\"\n",
    "intervals = \"/gpfs/gibbs/pi/brueckner/ken_rotation/intervals_3.bed\"   #Intersected bed file\n",
    "exome_intervals = hl.import_locus_intervals(intervals, reference_genome='GRCh38')\n",
    "mt = hl.read_matrix_table(mt_input)\n",
    "#mt = mt.annotate_rows(old_locus=mt.locus) #If lifting over\n",
    "\n",
    "# Liftover from hg19 to hg38\n",
    "#rg37 = hl.get_reference('GRCh37')  \n",
    "#rg38 = hl.get_reference('GRCh38')  \n",
    "#rg37.add_liftover(\"/gpfs/gibbs/pi/brueckner/ken_rotation/references_grch37_to_grch38.over.chain.gz\", rg38) \n",
    "#mt = mt.annotate_rows(new_locus=hl.liftover(mt.locus, 'GRCh38'))  \n",
    "#mt = mt.filter_rows(hl.is_defined(mt.new_locus))  \n",
    "#mt = mt.key_rows_by(locus=mt.new_locus, alleles=mt.alleles)  \n",
    "\n",
    "mt = mt.filter_rows(hl.is_defined(exome_intervals[mt.locus]), keep=True)\n",
    "mt.write(mt_output, overwrite = True)\n",
    "\n",
    "# Generate sample QC metrics\n",
    "mt_input = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/split_intervals_acc.mt\"\n",
    "mt_output = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/split_sampleqc_acc.mt\"\n",
    "mt = hl.read_matrix_table(mt_input)\n",
    "mt = hl.sample_qc(mt, name='sample_qc')\n",
    "mt.write(mt_output, overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "vep_json = '/gpfs/gibbs/pi/brueckner/14_v0/vep85-loftee-ruddle-b38.json'\n",
    "\n",
    "#mt_input =\"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/split_intervals_acc.mt\"\n",
    "#mt_output =\"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/vep_acc.mt\"\n",
    "\n",
    "#print('Reads in the following matrix table: \\n' + mt_input + '\\n')\n",
    "mt = hl.read_matrix_table(mt_input)\n",
    "\n",
    "print('Writes out to the following matrix table: \\n' + mt_output + '\\n')\n",
    "\n",
    "mt_vep = hl.vep(mt, vep_json)\n",
    "mt_vep.write(mt_output, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4dc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter out PCGC4 Samples ###\n",
    "rf = open(\"/gpfs/gibbs/pi/brueckner/ken_rotation/pilot_info_for_ken.tsv\", 'r+')   #Open the file for reading and writing\n",
    "rf_line = []   #Initialize a list\n",
    "for line in rf:   #Interate through all lines\n",
    "    line_s = line.split('\\t')   #Split with a tab delimeter\n",
    "    rf_line.append(line_s)   #Append each line to the initialized list\n",
    "rf_line = rf_line[1:]\n",
    "new_lis = []  #LIST OF NON PCGC4\n",
    "\n",
    "for i in rf_line:\n",
    "    batch.append(i[3])\n",
    "    if i[3]!=\"NPCGC\\n\":\n",
    "        new_lis.append(i[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_acc = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/vep_acc.mt\"\n",
    "mt_a = hl.read_matrix_table(mt_acc)\n",
    "mt_yale = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/vep.mt\"\n",
    "mt_y = hl.read_matrix_table(mt_yale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c374c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove PCGC4 samples\n",
    "mt_a = mt_a.filter_cols(hl.set(new_lis).contains(mt_a.s))\n",
    "mt_y = mt_y.filter_cols(hl.set(new_lis).contains(mt_y.s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_a = mt_a.filter_rows(hl.agg.any(mt_a.GT.is_non_ref()))\n",
    "mt_y = mt_y.filter_rows(hl.agg.any(mt_y.GT.is_non_ref()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f07f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_a = hl.sample_qc(mt_a, name='sample_qc')\n",
    "mt_y = hl.sample_qc(mt_y, name='sample_qc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_a=mt_a.annotate_entries(AB = (mt_a.AD[1]/(mt_a.AD[0]+mt_a.AD[1])))\n",
    "#mt_a = mt_a.filter_entries(mt_a.GT.is_het())\n",
    "mt_y=mt_y.annotate_entries(AB = (mt_y.AD[1]/(mt_y.AD[0]+mt_y.AD[1])))\n",
    "#mt_y = mt_y.filter_entries(mt_y.GT.is_het())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7cea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for variant concordance, not call concordance\n",
    "# Compute variants unique in each dataset\n",
    "ds1 = mt_y.rows()   #yale\n",
    "ds2 = mt_a.rows()  #acc\n",
    "\n",
    "# unique in dataset1 -- filter out any variants in ds2\n",
    "ds1_unique = ds1.anti_join(ds2)\n",
    "\n",
    "# unique in dataset2 -- filter out any variants in ds1\n",
    "ds2_unique = ds2.anti_join(ds1)\n",
    "\n",
    "ds_shared = ds1.join(ds2)\n",
    "\n",
    "print(\"There are \"+ str(ds1_unique.count()) + \" unique variants present in the Yale dataset.\")\n",
    "print(\"There are \"+ str(ds2_unique.count()) + \" unique variants present in the ACC dataset.\")\n",
    "print(\"There are \"+ str(ds_shared.count()) + \" shared variants.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique = mt_y.filter_rows(hl.is_defined(ds1_unique[mt_y.row_key]))\n",
    "a_unique = mt_a.filter_rows(hl.is_defined(ds2_unique[mt_a.row_key]))\n",
    "shared = mt_y.filter_rows(hl.is_defined(ds_shared[mt_y.row_key]))\n",
    "shared_a = mt_a.filter_rows(hl.is_defined(ds_shared[mt_a.row_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique = hl.sample_qc(y_unique, name='sample_qc')\n",
    "a_unique = hl.sample_qc(a_unique, name='sample_qc')\n",
    "shared = hl.sample_qc(shared, name='sample_qc')\n",
    "shared_a = hl.sample_qc(shared_a, name='sample_qc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfd33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique = hl.variant_qc(y_unique, name='variant_qc')\n",
    "a_unique = hl.variant_qc(a_unique, name='variant_qc')\n",
    "shared = hl.variant_qc(shared, name='variant_qc')\n",
    "shared_a = hl.variant_qc(shared_a, name='variant_qc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60819e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique=y_unique.annotate_entries(AB = (y_unique.AD[1]/(y_unique.AD[0]+y_unique.AD[1])))\n",
    "y_unique = y_unique.filter_entries(y_unique.GT.is_het())\n",
    "\n",
    "a_unique=a_unique.annotate_entries(AB = (a_unique.AD[1]/(a_unique.AD[0]+a_unique.AD[1])))\n",
    "a_unique = a_unique.filter_entries(a_unique.GT.is_het())\n",
    "\n",
    "shared=shared.annotate_entries(AB = (shared.AD[1]/(shared.AD[0]+shared.AD[1])))\n",
    "shared = shared.filter_entries(shared.GT.is_het())\n",
    "\n",
    "shared_a=shared_a.annotate_entries(AB = (shared_a.AD[1]/(shared_a.AD[0]+shared_a.AD[1])))\n",
    "shared_a = shared_a.filter_entries(shared_a.GT.is_het())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique = y_unique.annotate_rows(mean_AB = hl.agg.mean(y_unique.AB))\n",
    "a_unique = a_unique.annotate_rows(mean_AB = hl.agg.mean(a_unique.AB))\n",
    "shared = shared.annotate_rows(mean_AB = hl.agg.mean(shared.AB))\n",
    "shared_a = shared_a.annotate_rows(mean_AB = hl.agg.mean(shared_a.AB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique = y_unique.annotate_rows(mean_DP = hl.agg.mean(y_unique.DP))\n",
    "a_unique = a_unique.annotate_rows(mean_DP = hl.agg.mean(a_unique.DP))\n",
    "shared = shared.annotate_rows(mean_DP = hl.agg.mean(shared.DP))\n",
    "shared_a = shared_a.annotate_rows(mean_DP = hl.agg.mean(shared_a.DP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitler PASS variants\n",
    "\n",
    "y_unique_p = y_unique.filter_rows(hl.len(y_unique.filters) == 0)\n",
    "a_unique_p = a_unique.filter_rows(hl.len(a_unique.filters) == 0)\n",
    "shared_p = shared.filter_rows(hl.len(shared.filters) == 0)\n",
    "\n",
    "y_unique_np = y_unique.filter_rows(hl.len(y_unique.filters) != 0)\n",
    "a_unique_np = a_unique.filter_rows(hl.len(a_unique.filters) != 0)\n",
    "shared_np = shared.filter_rows(hl.len(shared.filters) != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674452ef",
   "metadata": {},
   "source": [
    "# Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DP PER GENOTYPE\n",
    "dp_hist = y_unique_p.aggregate_entries(hl.expr.aggregators.hist(y_unique_p.AB, 0, 1, 30))\n",
    "p = hl.plot.histogram(dp_hist, title='Distribution of AB (Yale-Unique (PASS))')\n",
    "\n",
    "fig = p\n",
    "fig.xaxis.axis_label = 'Allele Balance'\n",
    "fig.yaxis.axis_label = 'Frequency'\n",
    "\n",
    "fig.title.align = \"center\"\n",
    "fig.title.text_color = \"black\"\n",
    "fig.title.text_font_size = \"20px\"\n",
    "\n",
    "fig.x_range.start = 0\n",
    "fig.x_range.end = 1\n",
    "\n",
    "fig.background_fill_color = \"white\"\n",
    "\n",
    "fig.xgrid.grid_line_color = None\n",
    "fig.ygrid.grid_line_color = None\n",
    "\n",
    "fig.xaxis.axis_label_text_font_size = \"17px\"\n",
    "fig.xaxis.axis_label_text_color = \"black\"\n",
    "fig.xaxis.axis_label_text_font_style = \"normal\"\n",
    "\n",
    "fig.yaxis.axis_label_text_font_size = \"17px\"\n",
    "fig.yaxis.axis_label_text_color = \"black\"\n",
    "fig.yaxis.axis_label_text_font_style = \"normal\"\n",
    "\n",
    "fig.outline_line_width = 0\n",
    "fig.outline_line_color = \"white\"\n",
    "\n",
    "fig.legend.border_line_width = 0\n",
    "fig.legend.label_width= 0\n",
    "fig.legend.label_height= 0\n",
    "#fig.legend.location = \"top_left\"\n",
    "fig.legend.visible = False\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DP PER VARIANT\n",
    "p = hl.plot.histogram(shared_np.mean_AB, title='Distribution of AB per variant (Shared (NO PASS))', bins = 30)\n",
    "\n",
    "fig = p\n",
    "fig.xaxis.axis_label = 'AB'\n",
    "fig.yaxis.axis_label = 'Frequency'\n",
    "\n",
    "fig.title.align = \"center\"\n",
    "fig.title.text_color = \"black\"\n",
    "fig.title.text_font_size = \"20px\"\n",
    "\n",
    "fig.x_range.end = 1\n",
    "#fig.x_range.start = 0.7\n",
    "#fig.y_range.end = 5000\n",
    "\n",
    "fig.background_fill_color = \"white\"\n",
    "\n",
    "fig.xgrid.grid_line_color = None\n",
    "fig.ygrid.grid_line_color = None\n",
    "\n",
    "fig.xaxis.axis_label_text_font_size = \"17px\"\n",
    "fig.xaxis.axis_label_text_color = \"black\"\n",
    "fig.xaxis.axis_label_text_font_style = \"normal\"\n",
    "\n",
    "fig.yaxis.axis_label_text_font_size = \"17px\"\n",
    "fig.yaxis.axis_label_text_color = \"black\"\n",
    "fig.yaxis.axis_label_text_font_style = \"normal\"\n",
    "\n",
    "fig.outline_line_width = 0\n",
    "fig.outline_line_color = \"white\"\n",
    "\n",
    "fig.legend.border_line_width = 0\n",
    "fig.legend.label_width= 0\n",
    "fig.legend.label_height= 0\n",
    "#fig.legend.location = \"top_left\"\n",
    "fig.legend.visible = False\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_plot = hl.plot.scatter(shared_a.sample_qc_new.r_het_hom_var, shared_a.sample_qc_new.n_snp, \n",
    "                           #colors=color_mapper,\n",
    "                           title ='n_snps vs Het/Hom (Shared)')\n",
    "\n",
    "#bokeh.models.mappers.ColorMapper or Mapping[str, bokeh.models.mappers.ColorMapper]\n",
    "\n",
    "fig = var_plot\n",
    "fig.xaxis.axis_label = 'Het/Hom Ratio'\n",
    "fig.yaxis.axis_label = 'Number of variants'\n",
    "\n",
    "#fig.y_range.start = 2.5\n",
    "#fig.y_range.end = 3000\n",
    "#fig.x_range.start = 25000\n",
    "fig.x_range.end = 2.2\n",
    "\n",
    "fig.title.align = \"center\"\n",
    "fig.title.text_color = \"black\"\n",
    "fig.title.text_font_size = \"20px\"\n",
    "\n",
    "fig.background_fill_color = \"white\"\n",
    "\n",
    "fig.xgrid.grid_line_color = None\n",
    "fig.ygrid.grid_line_color = None\n",
    "\n",
    "fig.xaxis.axis_label_text_font_size = \"17px\"\n",
    "fig.xaxis.axis_label_text_color = \"black\"\n",
    "fig.xaxis.axis_label_text_font_style = \"normal\"\n",
    "\n",
    "fig.yaxis.axis_label_text_font_size = \"17px\"\n",
    "fig.yaxis.axis_label_text_color = \"black\"\n",
    "fig.yaxis.axis_label_text_font_style = \"normal\"\n",
    "\n",
    "fig.outline_line_width = 0\n",
    "fig.outline_line_color = \"white\"\n",
    "\n",
    "'''fig.legend.border_line_width = 0\n",
    "fig.legend.label_width= 0\n",
    "fig.legend.label_height= 0\n",
    "#fig.legend.location = \"top_left\"\n",
    "#fig.legend.visible = False'''\n",
    "\n",
    "hl.plot.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff9ced0",
   "metadata": {},
   "source": [
    "# Concordance between call sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, samples, variants = hl.concordance(shared, shared_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_homref_right_homvar = summary[2][4]\n",
    "left_het_right_missing = summary[3][1]\n",
    "left_het_right_something_else = sum(summary[3][:]) - summary[3][3]\n",
    "total_concordant = summary[2][2] + summary[3][3] + summary[4][4]\n",
    "total_discordant = sum([sum(s[2:]) for s in summary[2:]]) - total_concordant\n",
    "\n",
    "samples.export(\"samples.tsv\", delimiter = \"\\t\")\n",
    "variants.export(\"variants.tsv\", delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5dc439",
   "metadata": {},
   "source": [
    "# Functional Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0eaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Sort VEP transcript consequences array \n",
    "# annotate with desired VEP annotations as new row fields\n",
    "# add gnomad \n",
    "# drop parent VEP struct (optional)\n",
    "# write new matrix table \n",
    "##############################################################################\n",
    "\n",
    "ref_data = '/gpfs/gibbs/pi/brueckner/hail_resources/combined_reference_data_grch38.ht'\n",
    "\n",
    "################################\n",
    "# specify inputs and outputs \n",
    "################################\n",
    "#mt_input = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/vep.mt\" # specify the matrix table you wish to annotate. At minimum should already be annotated with VEP. \n",
    "#mt_output = \"/gpfs/gibbs/pi/brueckner/ken_rotation/output/consortium_new/vep_r1.mt\"\n",
    "\n",
    "################################\n",
    "#mt = hl.read_matrix_table(mt_input)\n",
    "#print('Reads in the following matrix table: \\n' + mt_input + '\\n')\n",
    "#print('count of the input dataframe:' + str(mt.count()) + '\\n')\n",
    "mt = y_unique_p\n",
    "\n",
    "ref_data_ht = hl.read_table(ref_data)\n",
    "print('Reads in the following hail table for annotation: \\n' + ref_data + '\\n')\n",
    "\n",
    "################################\n",
    "# add annotations (from vep)\n",
    "################################\n",
    "\n",
    "# Sort VEP array annotations \n",
    "##  sort transcript consequences \n",
    "mt = mt.annotate_rows(\n",
    "    sortedTranscriptConsequences=get_expr_for_vep_sorted_transcript_consequences_array(vep_root=mt.vep)\n",
    ")\n",
    "\n",
    "# Annotate rows with index zero element of array sorted transcript annotations \n",
    "# Annotate rows with additional fields from VEP parent structure before dropping\n",
    "# Annotate rows with gnomad AF (Assign 0.0 to variants without a gnomAD frequency)\n",
    "mt = mt.annotate_rows(\n",
    "    gene_symbol=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].gene_symbol, hl.missing(hl.tstr)), \n",
    "    major_consequence=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].major_consequence, hl.missing(hl.tstr)), \n",
    "    hgvs=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].hgvs, hl.missing(hl.tstr)), \n",
    "    category=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].category, hl.missing(hl.tstr)), \n",
    "    canonical=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].canonical, -1),\n",
    "    polyphen_pred=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].polyphen_prediction, hl.missing(hl.tstr)),\n",
    "    sift_pred=hl.if_else(mt.sortedTranscriptConsequences.size() > 0, mt.sortedTranscriptConsequences[0].sift_prediction, hl.missing(hl.tstr)),\n",
    "    most_severe_consequence = mt.vep.most_severe_consequence,\n",
    "    variant_class = mt.vep.variant_class,\n",
    "    gene_id_array = mt.vep.transcript_consequences.gene_id,\n",
    "    gene_symbol_array = mt.vep.transcript_consequences.gene_symbol,\n",
    "    #gnomad_af = hl.if_else(hl.is_defined(ref_data_ht[mt.row_key]), ref_data_ht[mt.row_key].gnomad_exomes.AF, 0.0)\n",
    "    gnomad_af =  hl.if_else(hl.is_defined(ref_data_ht[mt.row_key]),(hl.if_else(hl.is_defined(ref_data_ht[mt.row_key].gnomad_exomes.AF),ref_data_ht[mt.row_key].gnomad_exomes.AF, 0.0)), 0.0),\n",
    "    meta_svm =  hl.if_else(hl.is_defined(ref_data_ht[mt.row_key]),(hl.if_else(hl.is_defined(ref_data_ht[mt.row_key].dbnsfp.MetaSVM_pred),ref_data_ht[mt.row_key].dbnsfp.MetaSVM_pred, 'na')), 'na'),\n",
    "    cadd =  hl.if_else(hl.is_defined(ref_data_ht[mt.row_key]),(hl.if_else(hl.is_defined(ref_data_ht[mt.row_key].cadd.PHRED),ref_data_ht[mt.row_key].cadd.PHRED, 0.0)), 0.0)\n",
    ")\n",
    "\n",
    "\n",
    "# drop parent vep structure \n",
    "mt = mt.drop(mt.vep)\n",
    "\n",
    "################################\n",
    "# write new matrix table\n",
    "################################\n",
    "\n",
    "print('Writes out to the following matrix table: \\n' + mt_output + '\\n')\n",
    "mt.write(mt_output, overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mt_input = vep_r1c1_mt\n",
    "#mt = hl.read_matrix_table(mt_input)\n",
    "#Could not save the vep_r1. Hence use the variable from above\n",
    "\n",
    "#print('Reads in the following matrix table: \\n' + mt_input)\n",
    "print('count of the input dataframe:' + str(mt.count()) + '\\n')\n",
    "\n",
    "# mt.describe()\n",
    "\n",
    "# How many of each variant class? \n",
    "vep_variant_class = mt.variant_class\n",
    "variant_class_dict = mt.aggregate_rows(hl.agg.counter(vep_variant_class))\n",
    "print('Number of each variant class:')\n",
    "pprint.pprint(variant_class_dict) \n",
    "print('\\n')\n",
    "\n",
    "# How many of each consequence type?\n",
    "vep_most_severe_consequence = mt.most_severe_consequence\n",
    "most_severe_term_dict = mt.aggregate_rows(hl.agg.counter(vep_most_severe_consequence))\n",
    "print('Number of each consequence type:')\n",
    "pprint.pprint(most_severe_term_dict)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for LOF\n",
    "\n",
    "y_unique_lof_p = y_unique_p.filter_rows(hl.set(['frameshift_variant', 'stop_gained', \"splice_acceptor_variant\", \"splice_donor_variant\"]).contains(y_unique_p.most_severe_consequence))\n",
    "a_unique_lof_p = a_unique_p.filter_rows(hl.set(['frameshift_variant', 'stop_gained', \"splice_acceptor_variant\", \"splice_donor_variant\"]).contains(a_unique_p.most_severe_consequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the SNPs and Indels for analysis\n",
    "y_unique_snp = y_unique.filter_rows(hl.set([\"SNV\"]).contains(y_unique.variant_class))\n",
    "y_unique_indel = y_unique.filter_rows(hl.set([\"insertion\", \"deletion\"]).contains(y_unique.variant_class))\n",
    "\n",
    "a_unique_snp = a_unique.filter_rows(hl.set([\"SNV\"]).contains(a_unique.variant_class))\n",
    "a_unique_indel = a_unique.filter_rows(hl.set([\"insertion\", \"deletion\"]).contains(a_unique.variant_class))\n",
    "\n",
    "shared_snp = shared.filter_rows(hl.set([\"SNV\"]).contains(shared.variant_class))\n",
    "shared_indel = shared.filter_rows(hl.set([\"insertion\", \"deletion\"]).contains(shared.variant_class))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
